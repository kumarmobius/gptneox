name: Build TinyGPTNeoX Model
description: Builds an untrained TinyGPTNeoX model, saves weights, JSON config, and model code.
inputs:
  - name: vocab_size
    type: Integer
  - name: emb_dim
    type: Integer
  - name: num_layers
    type: Integer
    default: 4
  - name: num_heads
    type: Integer
    default: 8
  - name: ff_mult
    type: Integer
    default: 4
  - name: dropout
    type: Float
    default: 0.0
  - name: rotary_pct
    type: Float
    default: 0.25
  - name: max_seq_len
    type: Integer
    default: 512
outputs:
  - name: model_weights
    type: Model
  - name: model_config
    type: Data
  - name: model_py
    type: Data
implementation:
  container:
    image: python:3.9
    command:
      - python3
      - -u
      - -c
      - |
        import os, json, argparse, subprocess, torch, torch.nn as nn, torch.nn.functional as F, math

        # Install dependencies
        subprocess.run(["pip", "install", "--quiet", "torch"], check=True)

        # =====================
        # Model Code as String
        # =====================
        MODEL_PY = r"""
import math
import torch
import torch.nn as nn
import torch.nn.functional as F

def rotate_half(x: torch.Tensor) -> torch.Tensor:
    x1 = x[..., ::2]
    x2 = x[..., 1::2]
    return torch.stack((-x2, x1), dim=-1).reshape_as(x)

def apply_rotary_pos_emb(q, k, sin, cos):
    q_rot = (q * cos) + (rotate_half(q) * sin)
    k_rot = (k * cos) + (rotate_half(k) * sin)
    return q_rot, k_rot

def make_rotary_sin_cos(seq_len, rotary_dim, device, dtype=torch.float32):
    inv_freq = 1.0 / (10000 ** (torch.arange(0, rotary_dim, 2, device=device, dtype=dtype) / rotary_dim))
    positions = torch.arange(seq_len, device=device, dtype=dtype)
    freqs = torch.einsum("i,j->ij", positions, inv_freq)
    emb = torch.cat((freqs, freqs), dim=-1)
    return emb.sin(), emb.cos()

class MaskedMultiHeadAttention(nn.Module):
    def __init__(self, emb_dim, num_heads, rotary_pct=0.25, dropout=0.0):
        super().__init__()
        assert emb_dim % num_heads == 0
        self.emb_dim = emb_dim
        self.num_heads = num_heads
        self.head_dim = emb_dim // num_heads
        self.rotary_pct = rotary_pct
        self.rotary_dim = int(self.head_dim * rotary_pct)
        if self.rotary_dim % 2 != 0:
            self.rotary_dim -= 1
        self.qkv_proj = nn.Linear(emb_dim, emb_dim * 3, bias=False)
        self.out_proj = nn.Linear(emb_dim, emb_dim, bias=False)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, sin=None, cos=None, attn_mask=None, past_kv=None):
        B, T, E = x.shape
        qkv = self.qkv_proj(x).view(B, T, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)
        q, k, v = qkv[0], qkv[1], qkv[2]
        if self.rotary_dim > 0:
            q_rot, q_pass = q[..., :self.rotary_dim], q[..., self.rotary_dim:]
            k_rot, k_pass = k[..., :self.rotary_dim], k[..., self.rotary_dim:]
            sin = sin.view(1, 1, T, self.rotary_dim)
            cos = cos.view(1, 1, T, self.rotary_dim)
            q_rot, k_rot = apply_rotary_pos_emb(q_rot, k_rot, sin, cos)
            q = torch.cat((q_rot, q_pass), dim=-1)
            k = torch.cat((k_rot, k_pass), dim=-1)
        if past_kv is not None:
            past_k, past_v = past_kv
            k = torch.cat([past_k, k], dim=2)
            v = torch.cat([past_v, v], dim=2)
        present_kv = (k, v)
        scale = 1.0 / math.sqrt(self.head_dim)
        attn_scores = torch.matmul(q, k.transpose(-2, -1)) * scale
        if attn_mask is None:
            q_len, k_len = q.size(2), k.size(2)
            mask = torch.ones((q_len, k_len), device=x.device, dtype=torch.bool).tril()
            attn_mask = mask.unsqueeze(0).unsqueeze(0)
        attn_scores = attn_scores.masked_fill(~attn_mask, float("-inf"))
        attn_probs = self.dropout(torch.softmax(attn_scores, dim=-1))
        out = torch.matmul(attn_probs, v).transpose(1, 2).contiguous().view(B, T, E)
        return self.out_proj(out), present_kv

class FeedForward(nn.Module):
    def __init__(self, emb_dim, ff_mult=4, dropout=0.0):
        super().__init__()
        self.fc1 = nn.Linear(emb_dim, emb_dim * ff_mult)
        self.act = nn.GELU()
        self.fc2 = nn.Linear(emb_dim * ff_mult, emb_dim)
        self.dropout = nn.Dropout(dropout)
    def forward(self, x):
        return self.dropout(self.fc2(self.act(self.fc1(x))))

class NeoXBlock(nn.Module):
    def __init__(self, emb_dim, num_heads, rotary_pct=0.25, ff_mult=4, dropout=0.0):
        super().__init__()
        self.ln_attn = nn.LayerNorm(emb_dim)
        self.ln_ff = nn.LayerNorm(emb_dim)
        self.attn = MaskedMultiHeadAttention(emb_dim, num_heads, rotary_pct=rotary_pct, dropout=dropout)
        self.ff = FeedForward(emb_dim, ff_mult=ff_mult, dropout=dropout)
    def forward(self, x, sin=None, cos=None, attn_mask=None, past_kv=None):
        norm_x = self.ln_attn(x)
        attn_out, present_kv = self.attn(norm_x, sin=sin, cos=cos, attn_mask=attn_mask, past_kv=past_kv)
        x = x + attn_out
        norm_x = self.ln_ff(x)
        ff_out = self.ff(norm_x)
        x = x + ff_out
        return x, present_kv

class TinyGPTNeoX(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        vocab_size, emb_dim = cfg["vocab_size"], cfg["emb_dim"]
        num_layers = cfg.get("num_layers", 4)
        num_heads = cfg.get("num_heads", 8)
        rotary_pct = cfg.get("rotary_pct", 0.25)
        self.emb = nn.Embedding(vocab_size, emb_dim)
        self.pos_max = cfg.get("max_seq_len", 512)
        self.blocks = nn.ModuleList([
            NeoXBlock(emb_dim, num_heads, rotary_pct=rotary_pct, ff_mult=cfg.get("ff_mult",4), dropout=cfg.get("dropout",0.0))
            for _ in range(num_layers)
        ])
        self.final_ln = nn.LayerNorm(emb_dim)
        self.head = nn.Linear(emb_dim, vocab_size, bias=False)
        self.head.weight = self.emb.weight
        rotary_dim = self.blocks[0].attn.rotary_dim
        if rotary_dim > 0:
            sin, cos = make_rotary_sin_cos(self.pos_max, rotary_dim, device="cpu", dtype=torch.float32)
            self.register_buffer("sin_cache", sin, persistent=False)
            self.register_buffer("cos_cache", cos, persistent=False)
        else:
            self.sin_cache = self.cos_cache = None
    def forward(self, input_ids, past_kvs=None):
        B, T = input_ids.shape
        device = input_ids.device
        x = self.emb(input_ids)
        past_len = past_kvs[0][0].size(2) if past_kvs is not None else 0
        if self.sin_cache is not None:
            sin = self.sin_cache[past_len:past_len+T].to(x.device).to(x.dtype)
            cos = self.cos_cache[past_len:past_len+T].to(x.device).to(x.dtype)
        else:
            sin = cos = None
        new_kvs = []
        for i, block in enumerate(self.blocks):
            past_kv = past_kvs[i] if past_kvs is not None else None
            x, present_kv = block(x, sin=sin, cos=cos, past_kv=past_kv)
            new_kvs.append(present_kv)
        x = self.final_ln(x)
        logits = self.head(x)
        return logits, new_kvs
        """

        # =====================
        # Run Component
        # =====================
        def main():
            parser = argparse.ArgumentParser()
            parser.add_argument("--vocab_size", type=int, required=True)
            parser.add_argument("--emb_dim", type=int, required=True)
            parser.add_argument("--num_layers", type=int, required=True)
            parser.add_argument("--num_heads", type=int, required=True)
            parser.add_argument("--ff_mult", type=int, required=True)
            parser.add_argument("--dropout", type=float, required=True)
            parser.add_argument("--rotary_pct", type=float, required=True)
            parser.add_argument("--max_seq_len", type=int, required=True)
            parser.add_argument("--model_weights", required=True)
            parser.add_argument("--model_config", required=True)
            parser.add_argument("--model_py", required=True)
            args = parser.parse_args()

            cfg = {
                "vocab_size": args.vocab_size,
                "emb_dim": args.emb_dim,
                "num_layers": args.num_layers,
                "num_heads": args.num_heads,
                "ff_mult": args.ff_mult,
                "dropout": args.dropout,
                "rotary_pct": args.rotary_pct,
                "max_seq_len": args.max_seq_len
            }

            model = TinyGPTNeoX(cfg)
            device = "cuda" if torch.cuda.is_available() else "cpu"
            model.to(device)

            os.makedirs(os.path.dirname(args.model_weights) or ".", exist_ok=True)
            torch.save(model.state_dict(), args.model_weights)

            os.makedirs(os.path.dirname(args.model_config) or ".", exist_ok=True)
            with open(args.model_config, "w") as f:
                json.dump(cfg, f, indent=2)

            os.makedirs(os.path.dirname(args.model_py) or ".", exist_ok=True)
            with open(args.model_py, "w") as f:
                f.write(MODEL_PY)

            print(f"[INFO] Model weights saved to {args.model_weights}")
            print(f"[INFO] Model config saved to {args.model_config}")
            print(f"[INFO] Model code saved to {args.model_py}")

        if __name__ == "__main__":
            main()
    args:
      - --vocab_size
      - {inputValue: vocab_size}
      - --emb_dim
      - {inputValue: emb_dim}
      - --num_layers
      - {inputValue: num_layers}
      - --num_heads
      - {inputValue: num_heads}
      - --ff_mult
      - {inputValue: ff_mult}
      - --dropout
      - {inputValue: dropout}
      - --rotary_pct
      - {inputValue: rotary_pct}
      - --max_seq_len
      - {inputValue: max_seq_len}
      - --model_weights
      - {outputPath: model_weights}
      - --model_config
      - {outputPath: model_config}
      - --model_py
      - {outputPath: model_py}
